#   Country-info-finder

####    Video Demo:  <https://youtu.be/omZo4GDw_ck>
####    Description: This Python project leverages the Selenium library to automate data extraction from a web page. The program is designed to fetch specific information about countries, including their capitals, currencies, and primary languages, by interacting with a sample table hosted on the website <https://cosmocode.io/automation-practice-webtable>. Selenium is a powerful tool for web automation that allows for simulating user actions in a web browser, making it ideal for tasks that involve dynamic content. This script is particularly useful for anyone who needs to automate the process of collecting structured data from websites.The script begins by importing essential modules from Selenium, such as webdriver, which is used to control the browser, and By, which helps in locating elements on the page. Additionally, it imports Options and Service to configure the Chrome browser settings and specify the path to the ChromeDriver executable. One of the key configurations is the use of headless mode, a setting that allows Chrome to run in the background without displaying a graphical interface. This improves performance and reduces resource consumption, especially in automated workflows or server environments where a display is unnecessary. The headless mode is activated by adding the --headless argument to Chrome options. Other Chrome options include --no-sandbox, which is crucial for running the browser in certain Linux environments, and --disable-dev-shm-usage, which addresses potential issues related to shared memory. Once the ChromeDriver is configured with these options, the WebDriver is initialized and directed to open the target URL. The maximize_window() method ensures that the browser window is fully expanded, which can help in locating elements more reliably, especially when dealing with complex layouts.The core functionality of the program revolves around extracting data from the table on the webpage. After navigating to the site, the script locates the table using its HTML ID (countries). It then retrieves all the rows within the table by searching for elements tagged with <tr>. Each row is further broken down into individual cells (<td> elements) to access specific pieces of information.The program defines three main functions to extract data: get_currency(), get_capital(), and get_language(). These functions take a country name as input and iterate through the rows of the table to find a match in the second column, which contains country names. When a match is found, the corresponding data is extracted from the relevant column—currency from the fourth column, capital from the third column, and primary language from the fifth column. The functions return the extracted data, which can then be used or displayed as needed.The main() function serves as the entry point of the program. It calls the data extraction functions for a specified country and prints the results. In this example, the country is set to "Bangladesh." The script retrieves and displays the capital, currency, and primary language of Bangladesh, showcasing the effectiveness of the functions in extracting accurate data. The program’s modular design allows for easy customization, enabling users to input different country names or expand the script to extract additional types of data from the table.This project highlights the versatility of Selenium for web scraping and automation tasks. By automating browser interactions, the script efficiently handles the retrieval of structured data from a dynamic webpage. The use of headless browsing ensures that the process runs smoothly and efficiently, making it suitable for deployment in environments where resources are limited. Additionally, the program’s structure follows best practices for code organization and readability, making it easy to understand and modify.In summary, this Python automation project is a practical demonstration of how Selenium can be used to scrape and extract data from web pages. It provides a robust solution for tasks that require interaction with web elements and the extraction of specific information. By leveraging Selenium’s capabilities, the script automates a repetitive task, saving time and reducing the potential for human error. The use of headless Chrome, combined with a well-structured approach to data retrieval, ensures that the program is both efficient and effective, making it a valuable tool for anyone looking to automate data collection from websites.
